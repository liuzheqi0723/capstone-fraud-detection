{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3_Join datasets with further data clean.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/liuzheqi0723/capstone-fraud-detection/blob/YaoW/models/3_Join_datasets_with_further_data_clean.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Application for real-time fraudulent transaction detection**"
      ],
      "metadata": {
        "id": "7FOAfnlsuPBf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Load Datasets and Import Libraries"
      ],
      "metadata": {
        "id": "4lTUgrgKuZS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### import libraries ###\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "HfhpdteTuttB"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # # Run it if it is the first time you running this notebook.\n",
        "\n",
        "# # # Mount your google drive to colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "zRnbj4Tiu-ic",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38133f0a-137a-4f16-cfc0-8754cd7b49d1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Before you run the code below,\n",
        "# Please create a shortcut for the 'Capstone' folder from shared drive to your own Googledrive.\n",
        "\n",
        "\n",
        "clean_id = pd.read_csv('/content/drive/MyDrive/Capstone/Data/clean_train_id.csv')\n",
        "clean_id.name = 'clean_id'\n",
        "# clean_train_id.head()\n",
        "\n",
        "clean_trans = pd.read_csv('/content/drive/MyDrive/Capstone/Data/clean_train_trans.csv')\n",
        "clean_trans.name = 'clean_trans'\n",
        "# clean_trans.head()\n",
        "\n",
        "# Dataset is now stored in a Pandas Dataframe"
      ],
      "metadata": {
        "id": "L5yFUbXBvHG8"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Fill Nan values (Part A)"
      ],
      "metadata": {
        "id": "RoFFULz0zp7L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### A.fill with string and constant values. "
      ],
      "metadata": {
        "id": "ncqi2nhDqXq1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**'clean_id' dataset:**\n",
        "1. All the Nans in columns with data type of **'object'** will be filled with string **'NA_'**.\n",
        "2. All the Nans in columns with data type of **'float'** will be filled with mean value of the column using [sklearn.impute](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html#sklearn.impute.SimpleImputer).<br>To prevent from data leakage, we will do this after seperating the training and testing datasets."
      ],
      "metadata": {
        "id": "r4zDs_6Fz80V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clean_id.drop(columns=['Unnamed: 0'], inplace=True) # drop index col"
      ],
      "metadata": {
        "id": "RQpOVavgI8Wg"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fill object cols with str 'NA'\n",
        "cols = clean_id.columns.to_list()\n",
        "for col in cols:\n",
        "  if clean_id[col].dtype == 'O':\n",
        "    clean_id[col].fillna('NA_', inplace=True)"
      ],
      "metadata": {
        "id": "4is2v-0DG5UK"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**'clean_trans' dataset:**\n",
        "\n"
      ],
      "metadata": {
        "id": "VNJ8FzOyN5Pe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clean_trans.drop(columns=['Unnamed: 0'], inplace=True) # drop index col"
      ],
      "metadata": {
        "id": "NI52WZikOTT1"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. All the Nans in columns with data type of **'object'** will be filled with string **'NA_'**."
      ],
      "metadata": {
        "id": "v0cc8V_5I36T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. For the Nans in columns with data type of **'float'** will be treated differently.<br><br>\n",
        "`    2.a fill with an unique value that has never appears in the column.`\n",
        "\n",
        "  >Vxxx: Vesta engineered rich features, including ranking, counting, and other entity relations.\n",
        "For example, how many times the payment card associated with a IP and email or address appeared in 24 hours time range, etc.\n",
        ">*Because the 'VXXX' columns are engineered features, the nan values indicate that the row do not belongs any category of the column, which is also an infomative message. *"
      ],
      "metadata": {
        "id": "9SuFAMX0JB7-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fill object cols with str 'NA'\n",
        "cols = clean_trans.columns.to_list()\n",
        "for col in cols: \n",
        "  if clean_trans[col].dtype == 'O': # condition 1\n",
        "    clean_trans[col].fillna('NA_', inplace=True)\n",
        "  elif str(col).startswith( 'V' ): # condition 2.a\n",
        "    clean_trans[col].fillna(-1, inplace=True)"
      ],
      "metadata": {
        "id": "2mXeyq3KPg0d"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##test and see if the code works\n",
        "#clean_trans['card4'].unique()"
      ],
      "metadata": {
        "id": "8LXql16AI-Kk"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "    2.b fill with **mean** value of the column using sklearn.impute.\n",
        "\n",
        ">card1 - card6: payment card information.Such as card type, card category, issue bank, country, etc.\n",
        "\n",
        ">addr: both addresses are for purchaser.\n",
        "addr1 as billing region.\n",
        "addr2 as billing country.\n",
        "\n",
        ">dist: distances between (not limited) billing address, mailing address, zip code, IP address, phone area, etc.\n",
        "\n",
        "*To prevent from data leakage, we will do this after seperating the training and testing datasets.*<br>\n",
        "\n",
        "    2.c fill with **most frequent value** in the column.\n",
        "  >C1-C14: counting, such as how many addresses are found to be associated with the payment card, etc. The actual meaning is masked.\n",
        "  \n",
        "  >D1-D15: timedelta, such as days between previous transaction, etc.\n",
        "\n",
        "  *To prevent from data leakage, we will do this after seperating the training and testing datasets.*<br>"
      ],
      "metadata": {
        "id": "jeDoGg9rJFg-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Join datasets, Save X_raw and y_raw\n",
        "\n"
      ],
      "metadata": {
        "id": "ZhA4IT1sYOOT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Join two dfs, to get a df ready for used in scikit learn.\n",
        "df_join = clean_id.merge(clean_trans, left_on='TransactionID', right_on='TransactionID')\n",
        "df_join\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "fHXGY8M9YT7E",
        "outputId": "6e1978d9-dfa7-4102-ba59-b919ff07c052"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        TransactionID  id_01     id_02  id_05  id_06  id_11     id_12  id_13  \\\n",
              "0             2987004    0.0   70787.0    NaN    NaN  100.0  NotFound    NaN   \n",
              "1             2987008   -5.0   98945.0    0.0   -5.0  100.0  NotFound   49.0   \n",
              "2             2987010   -5.0  191631.0    0.0    0.0  100.0  NotFound   52.0   \n",
              "3             2987011   -5.0  221832.0    0.0   -6.0  100.0  NotFound   52.0   \n",
              "4             2987016    0.0    7460.0    1.0    0.0  100.0  NotFound    NaN   \n",
              "...               ...    ...       ...    ...    ...    ...       ...    ...   \n",
              "134828        3577521  -15.0  145955.0    0.0    0.0  100.0  NotFound   27.0   \n",
              "134829        3577526   -5.0  172059.0    1.0   -5.0  100.0  NotFound   27.0   \n",
              "134830        3577529  -20.0  632381.0   -1.0  -36.0  100.0  NotFound   27.0   \n",
              "134831        3577531   -5.0   55528.0    0.0   -7.0  100.0  NotFound   27.0   \n",
              "134832        3577534  -45.0  339406.0  -10.0 -100.0  100.0  NotFound   27.0   \n",
              "\n",
              "        id_15     id_16  ...  V330  V331  V332 V333 V334 V335 V336 V337 V338  \\\n",
              "0         New  NotFound  ...   0.0   0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
              "1         New  NotFound  ...   0.0   0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
              "2       Found     Found  ...  -1.0  -1.0  -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0   \n",
              "3         New  NotFound  ...  -1.0  -1.0  -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0   \n",
              "4       Found     Found  ...   0.0   0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
              "...       ...       ...  ...   ...   ...   ...  ...  ...  ...  ...  ...  ...   \n",
              "134828  Found     Found  ...  -1.0  -1.0  -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0   \n",
              "134829    New  NotFound  ...   0.0   0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
              "134830    New  NotFound  ...  -1.0  -1.0  -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0   \n",
              "134831  Found     Found  ...   0.0   0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
              "134832    New  NotFound  ...  -1.0  -1.0  -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0   \n",
              "\n",
              "       V339  \n",
              "0       0.0  \n",
              "1       0.0  \n",
              "2      -1.0  \n",
              "3      -1.0  \n",
              "4       0.0  \n",
              "...     ...  \n",
              "134828 -1.0  \n",
              "134829  0.0  \n",
              "134830 -1.0  \n",
              "134831  0.0  \n",
              "134832 -1.0  \n",
              "\n",
              "[134833 rows x 414 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-39b046ce-0ed8-4ff1-b338-c8f5b783e11e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TransactionID</th>\n",
              "      <th>id_01</th>\n",
              "      <th>id_02</th>\n",
              "      <th>id_05</th>\n",
              "      <th>id_06</th>\n",
              "      <th>id_11</th>\n",
              "      <th>id_12</th>\n",
              "      <th>id_13</th>\n",
              "      <th>id_15</th>\n",
              "      <th>id_16</th>\n",
              "      <th>...</th>\n",
              "      <th>V330</th>\n",
              "      <th>V331</th>\n",
              "      <th>V332</th>\n",
              "      <th>V333</th>\n",
              "      <th>V334</th>\n",
              "      <th>V335</th>\n",
              "      <th>V336</th>\n",
              "      <th>V337</th>\n",
              "      <th>V338</th>\n",
              "      <th>V339</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2987004</td>\n",
              "      <td>0.0</td>\n",
              "      <td>70787.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>100.0</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>NaN</td>\n",
              "      <td>New</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2987008</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>98945.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>49.0</td>\n",
              "      <td>New</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2987010</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>191631.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>52.0</td>\n",
              "      <td>Found</td>\n",
              "      <td>Found</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2987011</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>221832.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-6.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>52.0</td>\n",
              "      <td>New</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2987016</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7460.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Found</td>\n",
              "      <td>Found</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>134828</th>\n",
              "      <td>3577521</td>\n",
              "      <td>-15.0</td>\n",
              "      <td>145955.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>27.0</td>\n",
              "      <td>Found</td>\n",
              "      <td>Found</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>134829</th>\n",
              "      <td>3577526</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>172059.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>27.0</td>\n",
              "      <td>New</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>134830</th>\n",
              "      <td>3577529</td>\n",
              "      <td>-20.0</td>\n",
              "      <td>632381.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-36.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>27.0</td>\n",
              "      <td>New</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>134831</th>\n",
              "      <td>3577531</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>55528.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-7.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>27.0</td>\n",
              "      <td>Found</td>\n",
              "      <td>Found</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>134832</th>\n",
              "      <td>3577534</td>\n",
              "      <td>-45.0</td>\n",
              "      <td>339406.0</td>\n",
              "      <td>-10.0</td>\n",
              "      <td>-100.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>27.0</td>\n",
              "      <td>New</td>\n",
              "      <td>NotFound</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>134833 rows × 414 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-39b046ce-0ed8-4ff1-b338-c8f5b783e11e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-39b046ce-0ed8-4ff1-b338-c8f5b783e11e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-39b046ce-0ed8-4ff1-b338-c8f5b783e11e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# #column dist1 and D11 only has Nan in df_join。\n",
        "# df_join=df_join.drop(columns=['dist1', 'D11'], inplace=False)"
      ],
      "metadata": {
        "id": "f-2U7DkT8DAb"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define X and y\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df_join.drop(columns=['TransactionID', 'isFraud'], inplace=False) # drop id and label\n",
        "y = df_join['isFraud']\n",
        "\n",
        "# X.dtypes.unique()"
      ],
      "metadata": {
        "id": "mSUP5-kry2FV"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Save 'X_raw.csv' and 'y_raw.csv' before get dummies\n"
      ],
      "metadata": {
        "id": "8VxbuKa60HoB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# X.to_csv('X_raw.csv')\n",
        "# !cp X_raw.csv \"drive/MyDrive/Capstone/Data/\"\n",
        "\n",
        "# y.to_csv('y_raw.csv')\n",
        "# !cp y_raw.csv \"drive/MyDrive/Capstone/Data/\""
      ],
      "metadata": {
        "id": "1nARs7ZN7kVb"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get Dummies\n",
        "\n",
        "Get dummies of the catogorical variables, does not improve the performance of ML Models. So the following part are all comment out.\n",
        "But we still keep all these codes for potencially used in the future."
      ],
      "metadata": {
        "id": "QdIwxv5nx7Bk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # define a function to get dummies for the catogorical cols.\n",
        "# def get_dummies(df: DataFrame, cols: list):\n",
        "#     '''\n",
        "#     Get the dummy values for the categorical columns.\n",
        "#     Append them to the input df and drop the original cols.\n",
        "\n",
        "#     df: data.\n",
        "#     cols: the name of the columns need to be converted.\n",
        "#     '''\n",
        "\n",
        "#     for col in cols:\n",
        "#         if col in df.columns:\n",
        "#             col_dummies = pd.get_dummies(data=df[col])\n",
        "#             df = pd.concat([df, col_dummies], axis=1)\n",
        "#             df = df.drop(col, axis=1)\n",
        "\n",
        "  \n",
        "#     return df"
      ],
      "metadata": {
        "id": "g_XVs_WUvhPW"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # there are many cols named as 'NA_'\n",
        "# # rename the duplicates named cols.\n",
        "\n",
        "# cols = pd.Series(X.columns)\n",
        "# dup_count = cols.value_counts()\n",
        "# for dup in cols[cols.duplicated()].unique():\n",
        "#     cols[cols[cols == dup].index.values.tolist()] = [dup + str(i) for i in range(1, dup_count[dup]+1)]\n",
        "\n",
        "# # run it twice, because newly named cols in last step got dups with ori not changed col names.\n",
        "# X.columns = cols\n",
        "# cols = pd.Series(X.columns)\n",
        "# dup_count = cols.value_counts()\n",
        "# for dup in cols[cols.duplicated()].unique():\n",
        "#     cols[cols[cols == dup].index.values.tolist()] = [dup + str(i) for i in range(1, dup_count[dup]+1)]\n",
        "\n",
        "# X.columns = cols"
      ],
      "metadata": {
        "id": "M7Oy2u8Q4dB0"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # test if there are still duplicates names in the df\n",
        "# uni_set = set()\n",
        "# for col in cols:\n",
        "#   if col not in uni_set:\n",
        "#     uni_set.add(col)\n",
        "#   else:\n",
        "#     print(col)\n",
        "\n",
        "# len(cols) - len(uni_set)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_SjZIPaa6GeU",
        "outputId": "702f10fb-0db2-4adf-c1f0-03e2e1978aee"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Fill Nan values (Part B)\n",
        "with mean and most frequent values. fit and train using pipeline."
      ],
      "metadata": {
        "id": "yXSLbZ0sdWFa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "    1.b fill with **mean** value of the column\n",
        ">numerical 'id_XX'\n",
        "\n",
        "    2.b fill with **mean** value of the column using sklearn.impute.\n",
        "\n",
        ">card1 - card6: payment card information.Such as card type, card category, issue bank, country, etc.\n",
        "\n",
        ">addr: both addresses are for purchaser.\n",
        "addr1 as billing region.\n",
        "addr2 as billing country.\n",
        "\n",
        ">dist: distances between (not limited) billing address, mailing address, zip code, IP address, phone area, etc.\n",
        "<br>\n",
        "\n",
        "    2.c fill with **most frequent value** in the column.\n",
        "  >C1-C14: counting, such as how many addresses are found to be associated with the payment card, etc. The actual meaning is masked.\n",
        "  \n",
        "  >D1-D15: timedelta, such as days between previous transaction, etc.\n",
        "<br>"
      ],
      "metadata": {
        "id": "pCTONV2TjHO_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "# Step 1:\n",
        "# filter out the cols with Nans.\n",
        "X_null = X.isnull().sum(axis=0).to_frame() # count Nans in every col.\n",
        "X_null.rename(columns={0: '#_Nans'}, inplace=True) # rename cols.\n",
        "X_NanCols = X_null[X_null['#_Nans']>0].index # get a series contains all the names of cols with Nan.\n",
        "\n",
        "X_fullCols = X_null[X_null['#_Nans']==0].index\n",
        "\n",
        "# make lists, indicating which stratage will be used in imputing the cols.\n",
        "cols_fill_mean = []\n",
        "cols_fill_freq = []\n",
        "\n",
        "for col in X_NanCols:\n",
        "  if str(col).startswith('C'): # cols C1-C1\n",
        "    cols_fill_freq.append(col)\n",
        "  elif str(col).startswith('D'): # cols D1-D15 and 'Device ...' which has been filled previously.\n",
        "    cols_fill_freq.append(col)\n",
        "  else:\n",
        "    cols_fill_mean.append(col) # cols id_XX and cols has already been filled with other startages earlier.\n",
        "\n",
        "# make all the cols still included in the following processing\n",
        "cols_fill_freq.extend(X_fullCols.to_list())"
      ],
      "metadata": {
        "id": "KKGvCSrntY5b"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cols_fill_freq"
      ],
      "metadata": {
        "id": "Pdj4lm_m1lC6"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2:\n",
        "# instantiate the imputers, within a pipeline\n",
        "# imputer imputes with the mean\n",
        "imp_mean = Pipeline(steps=[('imputer', SimpleImputer(missing_values=np.nan, strategy='mean'))])\n",
        "\n",
        "# imputer imputes with 'most_frequent'\n",
        "imp_freq = Pipeline(steps=[('imputer',SimpleImputer(missing_values=np.nan, strategy='most_frequent'))])\n",
        "\n",
        "\n",
        "# Step 3:\n",
        "# put the features list and the transformers together by col transformer.\n",
        "imp_preprocessor = ColumnTransformer(transformers=[('imp_mean', imp_mean, cols_fill_mean),\\\n",
        "                                                   ('imp_freq',imp_freq,cols_fill_freq)])"
      ],
      "metadata": {
        "id": "kABAiuaduxal"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4:\n",
        "# fit and trans the datasets with 'imp_preprocessor'.\n",
        "imp_preprocessor.fit(X_train)\n",
        "\n",
        "X_train = imp_preprocessor.transform(X_train)\n",
        "X_test = imp_preprocessor.transform(X_test)"
      ],
      "metadata": {
        "id": "qknOaIq9uyc7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 484
        },
        "outputId": "8d323610-cda3-42f1-ad86-4ff365086de9"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36m_get_column_indices\u001b[0;34m(X, key)\u001b[0m\n\u001b[1;32m    408\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m             \u001b[0mall_columns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'columns'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-cf2863fca24a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Step 4:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# fit and trans the datasets with 'imp_preprocessor'.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mimp_preprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimp_preprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/compose/_column_transformer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    640\u001b[0m         \u001b[0;31m# we use fit_transform to make sure to set sparse_output_ (for which we\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[0;31m# need the transformed data) to have consistent output type in predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 642\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    643\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/compose/_column_transformer.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    670\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_n_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_transformers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_column_callables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    673\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_remainder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/compose/_column_transformer.py\u001b[0m in \u001b[0;36m_validate_column_callables\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    350\u001b[0m                 \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m             \u001b[0mall_columns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m             \u001b[0mtransformer_to_input_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_column_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_columns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36m_get_column_indices\u001b[0;34m(X, key)\u001b[0m\n\u001b[1;32m    410\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m             raise ValueError(\n\u001b[0;32m--> 412\u001b[0;31m                 \u001b[0;34m\"Specifying the columns using strings is only \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    413\u001b[0m                 \u001b[0;34m\"supported for pandas DataFrames\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m             )\n",
            "\u001b[0;31mValueError\u001b[0m: Specifying the columns using strings is only supported for pandas DataFrames"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_val.shape"
      ],
      "metadata": {
        "id": "-fTMV8wHshCP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train"
      ],
      "metadata": {
        "id": "milYVf3ToMvs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train[1]\n"
      ],
      "metadata": {
        "id": "sRxmUdEzstJf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # output\n",
        "# X_train.tofile('X_train')\n",
        "# !cp X_train \"drive/MyDrive/Capstone/Data/\"\n",
        "\n",
        "# X_val.tofile('X_val')\n",
        "# !cp X_val \"drive/MyDrive/Capstone/Data/\"\n",
        "\n",
        "# X_test.tofile('X_test')\n",
        "# !cp X_test \"drive/MyDrive/Capstone/Data/\"\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UjvhTi65DAhB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# y_train.to_csv('y_train.csv')\n",
        "# !cp y_train.csv \"drive/MyDrive/Capstone/Data/\"\n",
        "\n",
        "# y_val.to_csv('y_val.csv')\n",
        "# !cp y_val.csv \"drive/MyDrive/Capstone/Data/\"\n",
        "\n",
        "# y_test.to_csv('y_test.csv')\n",
        "# !cp y_test.csv \"drive/MyDrive/Capstone/Data/\""
      ],
      "metadata": {
        "id": "M9eCl-H6GoWe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}