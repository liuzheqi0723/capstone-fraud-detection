{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3_Join datasets with further data clean.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/liuzheqi0723/capstone-fraud-detection/blob/main/models/3_Join_datasets_with_further_data_clean.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Application for real-time fraudulent transaction detection**"
      ],
      "metadata": {
        "id": "7FOAfnlsuPBf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Load Datasets and Import Libraries"
      ],
      "metadata": {
        "id": "4lTUgrgKuZS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### import libraries ###\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "HfhpdteTuttB"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # # Run it if it is the first time you running this notebook.\n",
        "\n",
        "# # # Mount your google drive to colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "zRnbj4Tiu-ic",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e3a7bd0-79ed-415a-dbd7-43ac89b39bb8"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Before you run the code below,\n",
        "# Please create a shortcut for the 'Capstone' folder from shared drive to your own Googledrive.\n",
        "\n",
        "\n",
        "clean_id = pd.read_csv('/content/drive/MyDrive/Capstone/Data/clean_train_id.csv')\n",
        "clean_id.name = 'clean_id'\n",
        "# clean_train_id.head()\n",
        "\n",
        "clean_trans = pd.read_csv('/content/drive/MyDrive/Capstone/Data/clean_train_trans.csv')\n",
        "clean_trans.name = 'clean_trans'\n",
        "# clean_trans.head()\n",
        "\n",
        "# Dataset is now stored in a Pandas Dataframe"
      ],
      "metadata": {
        "id": "L5yFUbXBvHG8"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Fill Nan values (Part A)"
      ],
      "metadata": {
        "id": "RoFFULz0zp7L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### A.fill with string and constant values. "
      ],
      "metadata": {
        "id": "ncqi2nhDqXq1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**'clean_id' dataset:**\n",
        "1. All the Nans in columns with data type of **'object'** will be filled with string **'NA_'**.\n",
        "2. All the Nans in columns with data type of **'float'** will be filled with mean value of the column using [sklearn.impute](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html#sklearn.impute.SimpleImputer).<br>To prevent from data leakage, we will do this after seperating the training and testing datasets."
      ],
      "metadata": {
        "id": "r4zDs_6Fz80V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clean_id.drop(columns=['Unnamed: 0'], inplace=True) # drop index col"
      ],
      "metadata": {
        "id": "RQpOVavgI8Wg"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fill object cols with str 'NA'\n",
        "cols = clean_id.columns.to_list()\n",
        "for col in cols:\n",
        "  if clean_id[col].dtype == 'O':\n",
        "    clean_id[col].fillna('NA_', inplace=True)"
      ],
      "metadata": {
        "id": "4is2v-0DG5UK"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**'clean_trans' dataset:**\n",
        "\n"
      ],
      "metadata": {
        "id": "VNJ8FzOyN5Pe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clean_trans.drop(columns=['Unnamed: 0'], inplace=True) # drop index col"
      ],
      "metadata": {
        "id": "NI52WZikOTT1"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. All the Nans in columns with data type of **'object'** will be filled with string **'NA_'**."
      ],
      "metadata": {
        "id": "v0cc8V_5I36T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. For the Nans in columns with data type of **'float'** will be treated differently.<br><br>\n",
        "`    2.a fill with an unique value that has never appears in the column.`\n",
        "\n",
        "  >Vxxx: Vesta engineered rich features, including ranking, counting, and other entity relations.\n",
        "For example, how many times the payment card associated with a IP and email or address appeared in 24 hours time range, etc.\n",
        ">*Because the 'VXXX' columns are engineered features, the nan values indicate that the row do not belongs any category of the column, which is also an infomative message. *"
      ],
      "metadata": {
        "id": "9SuFAMX0JB7-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fill object cols with str 'NA'\n",
        "cols = clean_trans.columns.to_list()\n",
        "for col in cols: \n",
        "  if clean_trans[col].dtype == 'O': # condition 1\n",
        "    clean_trans[col].fillna('NA_', inplace=True)\n",
        "  elif str(col).startswith( 'V' ): # condition 2.a\n",
        "    clean_trans[col].fillna(-1, inplace=True)"
      ],
      "metadata": {
        "id": "2mXeyq3KPg0d"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##test and see if the code works\n",
        "#clean_trans['card4'].unique()"
      ],
      "metadata": {
        "id": "8LXql16AI-Kk"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "    2.b fill with **mean** value of the column using sklearn.impute.\n",
        "\n",
        ">card1 - card6: payment card information.Such as card type, card category, issue bank, country, etc.\n",
        "\n",
        ">addr: both addresses are for purchaser.\n",
        "addr1 as billing region.\n",
        "addr2 as billing country.\n",
        "\n",
        ">dist: distances between (not limited) billing address, mailing address, zip code, IP address, phone area, etc.\n",
        "\n",
        "*To prevent from data leakage, we will do this after seperating the training and testing datasets.*<br>\n",
        "\n",
        "    2.c fill with **most frequent value** in the column.\n",
        "  >C1-C14: counting, such as how many addresses are found to be associated with the payment card, etc. The actual meaning is masked.\n",
        "  \n",
        "  >D1-D15: timedelta, such as days between previous transaction, etc.\n",
        "\n",
        "  *To prevent from data leakage, we will do this after seperating the training and testing datasets.*<br>"
      ],
      "metadata": {
        "id": "jeDoGg9rJFg-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Join datasets, Save X_raw and y_raw\n",
        "\n"
      ],
      "metadata": {
        "id": "ZhA4IT1sYOOT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# #column dist1 and D11 only has Nan in df_joinã€‚\n",
        "# df_join=df_join.drop(columns=['dist1', 'D11'], inplace=False)"
      ],
      "metadata": {
        "id": "f-2U7DkT8DAb"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define X and y\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df_join.drop(columns=['TransactionID', 'isFraud'], inplace=False) # drop id and label\n",
        "y = df_join['isFraud']\n",
        "\n",
        "# X.dtypes.unique()"
      ],
      "metadata": {
        "id": "mSUP5-kry2FV"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Save 'X_raw.csv' and 'y_raw.csv' before get dummies\n"
      ],
      "metadata": {
        "id": "8VxbuKa60HoB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# X.to_csv('X_raw.csv')\n",
        "# !cp X_raw.csv \"drive/MyDrive/Capstone/Data/\"\n",
        "\n",
        "# y.to_csv('y_raw.csv')\n",
        "# !cp y_raw.csv \"drive/MyDrive/Capstone/Data/\""
      ],
      "metadata": {
        "id": "1nARs7ZN7kVb"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get Dummies\n",
        "\n",
        "Get dummies of the catogorical variables, does not improve the performance of ML Models. So the following part are all comment out.\n",
        "But we still keep all these codes for potencially used in the future."
      ],
      "metadata": {
        "id": "QdIwxv5nx7Bk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # define a function to get dummies for the catogorical cols.\n",
        "# def get_dummies(df: DataFrame, cols: list):\n",
        "#     '''\n",
        "#     Get the dummy values for the categorical columns.\n",
        "#     Append them to the input df and drop the original cols.\n",
        "\n",
        "#     df: data.\n",
        "#     cols: the name of the columns need to be converted.\n",
        "#     '''\n",
        "\n",
        "#     for col in cols:\n",
        "#         if col in df.columns:\n",
        "#             col_dummies = pd.get_dummies(data=df[col])\n",
        "#             df = pd.concat([df, col_dummies], axis=1)\n",
        "#             df = df.drop(col, axis=1)\n",
        "\n",
        "  \n",
        "#     return df"
      ],
      "metadata": {
        "id": "g_XVs_WUvhPW"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # there are many cols named as 'NA_'\n",
        "# # rename the duplicates named cols.\n",
        "\n",
        "# cols = pd.Series(X.columns)\n",
        "# dup_count = cols.value_counts()\n",
        "# for dup in cols[cols.duplicated()].unique():\n",
        "#     cols[cols[cols == dup].index.values.tolist()] = [dup + str(i) for i in range(1, dup_count[dup]+1)]\n",
        "\n",
        "# # run it twice, because newly named cols in last step got dups with ori not changed col names.\n",
        "# X.columns = cols\n",
        "# cols = pd.Series(X.columns)\n",
        "# dup_count = cols.value_counts()\n",
        "# for dup in cols[cols.duplicated()].unique():\n",
        "#     cols[cols[cols == dup].index.values.tolist()] = [dup + str(i) for i in range(1, dup_count[dup]+1)]\n",
        "\n",
        "# X.columns = cols"
      ],
      "metadata": {
        "id": "M7Oy2u8Q4dB0"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # test if there are still duplicates names in the df\n",
        "# uni_set = set()\n",
        "# for col in cols:\n",
        "#   if col not in uni_set:\n",
        "#     uni_set.add(col)\n",
        "#   else:\n",
        "#     print(col)\n",
        "\n",
        "# len(cols) - len(uni_set)"
      ],
      "metadata": {
        "id": "_SjZIPaa6GeU"
      },
      "execution_count": 38,
      "outputs": []
    }
  ]
}