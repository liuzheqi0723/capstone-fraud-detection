# -*- coding: utf-8 -*-
"""Copy of 6_SML with non dummy.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14UW_YKiW_OmeVWjHbMdSkZUXUhd8YKX2

# **Application for real-time fraudulent transaction detection**
"""

### import libraries ###
import numpy as np
import pandas as pd

# # # Run it if it is the first time you running this notebook.

# # # Mount your google drive to colab
from google.colab import drive
drive.mount('/content/drive')

X = pd.read_csv('/content/drive/MyDrive/Capstone/Data/X_raw.csv')
X.drop(columns=['Unnamed: 0'], inplace=True) # drop index col

y_df = pd.read_csv('/content/drive/MyDrive/Capstone/Data/y_raw.csv')
y_df.drop(columns=['Unnamed: 0'], inplace=True) # drop index col

X.head()

y = y_df.to_numpy().reshape(-1) # convert type and reshape y


X = X.drop(columns=['dist1', 'D11'], inplace=False) # all nan values
X.name = 'X'

# print(X.shape)

categorical_columns = X.dtypes[X.dtypes == np.object].index.tolist() # list of columns with categorical variables

#create ordinal encoders for categorical variables
from sklearn.preprocessing import OrdinalEncoder
oe = OrdinalEncoder( dtype=int)
oe.fit(X[categorical_columns])
X[categorical_columns] = oe.transform(X[categorical_columns])

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test\
   = train_test_split(X, y, test_size=0.2, random_state=697)

X_null = X.isnull().sum(axis=0).to_frame() # count Nans in every col.
X_null.rename(columns={0: '#_Nans'}, inplace=True) # rename cols.

from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.compose import make_column_transformer

### Step 1:
# filter out the cols with Nans.
X_null = X.isnull().sum(axis=0).to_frame() # count Nans in every col.
X_null.rename(columns={0: '#_Nans'}, inplace=True) # rename cols.
X_NanCols = X_null[X_null['#_Nans']>0].index # get a series contains all the names of cols with Nan.

X_fullCols = X_null[X_null['#_Nans']==0].index   # column names without NA

# make lists, indicating which stratage will be used in imputing the cols.
cols_fill_mean = []
cols_fill_freq = []

for col in X_NanCols:
  if str(col).startswith('C'): # cols C1-C1
    cols_fill_freq.append(col)
  elif str(col).startswith('D'): # cols D1-D15 and 'Device ...' which has been filled previously.
    cols_fill_freq.append(col)
  else:
    cols_fill_mean.append(col) # cols id_XX and cols has already been filled with other startages earlier.

# make all the cols still included in the following processing
cols_fill_freq.extend(X_fullCols.to_list())

# Step 2:
# instantiate the imputers, within a pipeline
# imputer imputes with the mean
imp_mean = Pipeline(steps=[('imputer', SimpleImputer(missing_values=np.nan, strategy='mean'))])


# imputer imputes with 'most_frequent'
imp_freq = Pipeline(steps=[('imputer',SimpleImputer(missing_values=np.nan, strategy='most_frequent'))])

# Step 3:
# put the features list and the transformers together by col transformer.
imp_preprocessor = ColumnTransformer(transformers=[('imp_mean', imp_mean, cols_fill_mean),\
                                                   ('imp_freq',imp_freq,cols_fill_freq)])#,remainder='passthrough' )

# Step 4:
# fit and trans the datasets with 'imp_preprocessor'.
imp_preprocessor.fit(X_train)

X_train = imp_preprocessor.transform(X_train)
X_test = imp_preprocessor.transform(X_test)

# print(list(X.columns))

# np.save("X_train.npy", X_train)
# !cp X_train.npy "drive/MyDrive/Capstone/Data/"
# np.save("X_test.npy", X_test)
# !cp X_test.npy "drive/MyDrive/Capstone/Data/"

# np.save("y_train.npy", y_train)
# !cp y_train.npy "drive/MyDrive/Capstone/Data/"
# np.save("y_test.npy", y_test)
# !cp y_test.npy "drive/MyDrive/Capstone/Data/"

# from joblib import dump, load
# dump(imp_preprocessor, "nan_processor.joblib")
# !cp nan_processor.joblib "drive/MyDrive/Capstone/Data/"

"""## Supervised Machine Learning Models"""

from sklearn.metrics import accuracy_score

from sklearn.metrics import f1_score 
# F1 = 2 * (precision * recall) / (precision + recall)
# The recall is the ratio tp / (tp + fn)
# The recall is intuitively the ability of the classifier to find all the positive samples
# The precision is the ratio tp / (tp + fp) 
# The precision is intuitively the ability of the classifier not to label as positive a sample that is negative.

from sklearn.metrics import confusion_matrix
# [[true negatives, false negatives], 
# [true positives, false positives]].

from sklearn.neighbors import KNeighborsClassifier


def knn_clf(n_neighbors=5):
    
    print("The number of nearest neighbors is: {}".format(n_neighbors))
    
    knn = KNeighborsClassifier(n_neighbors=n_neighbors)
    knn.fit(X_train, y_train)
    y_pred = knn.predict(X_test)
    
    Acu = accuracy_score(y_test, y_pred)
    print("the accuracy score is: {}".format(round(Acu, 4)))

    F1 = f1_score(y_test, y_pred)
    print("the F1 score is: {}".format(round(F1, 4)))

    Mtrx = confusion_matrix(y_test, y_pred, labels = [0, 1])
    print("the confusion matrix score is:\n{}\n".format(Mtrx))
    

    return knn

from sklearn.tree import DecisionTreeClassifier

def tree_clf(max_depth=5):
    
    print("The number of depth is: {}".format(max_depth))
    
    dtree = DecisionTreeClassifier(max_depth=max_depth, criterion = 'entropy')
    dtree.fit(X_train, y_train)
    y_pred = dtree.predict(X_test)

    Acu = accuracy_score(y_test, y_pred)
    print("the accuracy score is: {}".format(round(Acu, 4)))

    F1 = f1_score(y_test, y_pred)
    print("the F1 score is: {}".format(round(F1, 4)))

    Mtrx = confusion_matrix(y_test, y_pred, labels = [0, 1])
    print("the confusion matrix score is:\n{}\n".format(Mtrx))

    return dtree

# best paras
best_DT = DecisionTreeClassifier(max_depth=16, criterion = 'entropy')
best_DT.fit(X_train, y_train)

"""### Random Forest"""

from sklearn.ensemble import RandomForestClassifier

def rf_clf(max_depth=5):
    
    print("The number of depth is: {}".format(max_depth))
    
    rf = RandomForestClassifier(max_depth = max_depth)
    rf.fit(X_train, y_train)
    y_pred = rf.predict(X_test)

    Acu = accuracy_score(y_test, y_pred)
    print("the accuracy score is: {}".format(round(Acu, 4)))

    F1 = f1_score(y_test, y_pred)
    print("the F1 score is: {}".format(round(F1, 4)))

    Mtrx = confusion_matrix(y_test, y_pred, labels = [0, 1])
    print("the confusion matrix score is:\n{}\n".format(Mtrx))

    return rf

#best paras
best_rf= RandomForestClassifier(max_depth = 35)
best_rf.fit(X_train, y_train)

"""### Support Vector Machines"""

from sklearn.svm import LinearSVC, LinearSVR, SVC, SVR

def svc_machine():
    
    svc_m = SVC()
    svc_m.fit(X_train, y_train)
    y_pred = svc_m.predict(X_test)

    Acu = accuracy_score(y_test, y_pred)
    print("the accuracy score is: {}".format(round(Acu, 4)))

    F1 = f1_score(y_test, y_pred)
    print("the F1 score is: {}".format(round(F1, 4)))

    Mtrx = confusion_matrix(y_test, y_pred, labels = [0, 1])
    print("the confusion matrix score is:\n{}\n".format(Mtrx))

    return svc_machine

# svc_machine()

"""### XGBooster"""

from xgboost import XGBClassifier


def xgb_clf(max_depth=5):
    
    print("The number of depth is: {}".format(max_depth))
    
    xgb = XGBClassifier(max_depth = max_depth)
    xgb.fit(X_train, y_train)
    y_pred = xgb.predict(X_test)

    Acu = accuracy_score(y_test, y_pred)
    print("the accuracy score is: {}".format(round(Acu, 4)))

    F1 = f1_score(y_test, y_pred)
    print("the F1 score is: {}".format(round(F1, 4)))

    Mtrx = confusion_matrix(y_test, y_pred, labels = [0, 1])
    print("the confusion matrix score is:\n{}\n".format(Mtrx))

    return xgb_clf

#best paras
best_xgb = XGBClassifier(max_depth = 30)
best_xgb.fit(X_train, y_train)

from joblib import dump, load
dump(best_DT, 'DesicionTree.joblib') 
dump(best_rf, 'RandomForest.joblib')

!cp DesicionTree.joblib "drive/MyDrive/Capstone/Data/"
!cp RandomForest.joblib "drive/MyDrive/Capstone/Data/"

best_xgb.save_model('XGBooster.bin')
!cp XGBooster.bin "drive/MyDrive/Capstone/Data/"

import xgboost as xgb

xgb.__version__

best_xgb.predict(X_test)

from sklearn.preprocessing import LabelEncoder
clf = xgb.XGBClassifier()
booster = xgb.Booster()
booster.load_model('XGBooster.json')
clf._Booster = booster
clf._le = LabelEncoder().fit([1, 0])

