# -*- coding: utf-8 -*-
"""Copy of 1_Datasets Understanding & Clean.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sKqiwrb4cAV81TkPuEKrqCMiAxlTN0-L

# **Application for real-time fraudulent transaction detection**
"""

# Commented out IPython magic to ensure Python compatibility.
### import libraries ###

import numpy as np
import pandas as pd

import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

"""##### A. Loading datasets from Google Drive
If you are running on Google Colab, and want to load datasets from Google Drive, using the following 2 cells.
"""



## If you are running this code using Google Colab,
## Run it if it is the first time you running this notebook.

# Mount your google drive to colab
from google.colab import drive
drive.mount('/content/drive')

# If you are loading the data from Google Drive, 
# Please create a shortcut for the 'Capstone' folder from shared drive to your own Googledrive.

path_train_id = "/content/drive/MyDrive/Capstone/Data/train_identity.csv"
raw_train_id = pd.read_csv(path_train_id)
raw_train_id.name = 'raw_train_id'
# raw_train_id.head()

path_train_trans = "/content/drive/MyDrive/Capstone/Data/train_transaction.csv"
raw_train_trans = pd.read_csv(path_train_trans)
raw_train_trans.name = 'raw_train_trans'
# raw_train_trans.head()

# Dataset is now stored in a Pandas Dataframe

"""##### B. Loading datasets from local drive.
Please comment out the code in the previous cells </br>
and use the code below to load the datasets from your local drive.
"""

# path_train_id = "" # paste the path here.
# raw_train_id = pd.read_csv(path_train_id)
# raw_train_id.name = 'raw_train_id'
# # raw_train_id.head()

# path_train_trans = "" # paste the path here.
# raw_train_trans = pd.read_csv(path_train_trans)
# raw_train_trans.name = 'raw_train_trans'
# # raw_train_trans.head()

# # Dataset is now stored in a Pandas Dataframe

"""### Understanding the raw data

#### Define functions to decribe DataFrame.
"""

# define functions used in the func 'df_description' in the next cell.
from pandas.core.frame import DataFrame

def null_info(df, axis=0):
    '''
    Return a DataFrame describes the Nans in df.
    df: DataFrame
     '''

    df_null = df.isnull().sum(axis=axis).to_frame()
    df_null.rename(columns={0: '#_Nans'}, inplace=True)
    if axis==0:
        num_total = df.shape[0]
    else:
        num_total = df.shape[1]

    df_null['%_Nans'] = round(df_null['#_Nans']/num_total*100, 2)
    df_null['#_Nans'] = df_null['#_Nans'].astype('int')

    return df_null



def dtype_info(df):
    '''
    Return a DataFrame describes the data type in df.
    df: DataFrame
    '''

    df_type = df.dtypes.to_frame()
    df_type.rename(columns={0: 'data_type'}, inplace=True)
    
    return df_type



def cols_info_df(df):
    '''
    Join the returns of func null_info() and dtype_info()
    Return a DataFrame describes the cols in df.
    df: DataFrame
    '''
    # generate decribing dfs using funcs defined above
    df_null = null_info(df) 
    df_type = dtype_info(df)

    # merge the decription dfs
    cols_info_df = df_null.merge(df_type, left_index=True, right_index=True,)  
    
    return cols_info_df

import missingno as msno

def df_description(df):
    '''
    Return basic info for the input df.
    df: DataFrame
    '''
    # print basic info
    print('For the DataFrame {}:\n'.format(df.name))
    print('The shape is: {}\n'.format(df.shape))

    # print cols info
    df_cols_info = cols_info_df(df) # func defined above
    print(df_cols_info,'\n')

    # if the number of cols is less than 50, print a graph describe the Nans.
    if len(df.columns)<50:
        print('The distribution of Nans:\n')
        msno.matrix(df)    # create a graph.

    return

def bin_chart(series, x_label:str, y_label:str):
    '''
    Plot a bin chart to show the %.
    '''

    n, bins, patches = plt.hist(series, 20, range=[0, 100], alpha=0.6)

    plt.xlabel(x_label)
    plt.ylabel(y_label)


    plt.show()

"""#### Describe the datasets"""

# take a look at sample data from 'raw_train_id'
raw_train_id.sample(5)

# use funcs defined in fomer cells to describe the dataset.
df_description(raw_train_id)

# density of Nans in cols
bin_chart(cols_info_df(raw_train_id)['%_Nans'], 'XX% of the values in the column are Nans', '# of cols')

# take a look at sample data from 'raw_train_trans'
raw_train_trans.sample(5)

# use defined funcs to describe the dataset.
df_description(raw_train_trans)

"""### Clean the datasets

#### Define functions to clean the datasets.
"""

from pandas.core.frame import DataFrame

def clean_df(df: DataFrame, \
             cols_drop: list, rows_drop: list, cols_to_category: list):
    '''
    Return a cleaned df.
    '''

    cleaned_df = df
    cleaned_df = cleaned_df.drop(columns=cols_drop) # drop cols
    cleaned_df = cleaned_df.drop(labels=rows_drop, axis=0) # drop rows

    for col in cols_to_category: # convert dtypes
        if col in cleaned_df.columns:
            cleaned_df[col] = cleaned_df[col].astype('category')

    return cleaned_df



def get_dummies(df: DataFrame, cols: list):
    '''
    Get the dummy values for the categorical columns.
    Append the it to the input df and drop the original cols.

    df: data.
    cols: the name of the columns need to be converted.
    '''

    for col in cols:
        if col in df.columns:
            col_dummies = pd.get_dummies(data=df[col])
            df = pd.concat([df, col_dummies], axis=1)
            df = df.drop(col, axis=1)

    
    return df

"""#### Clean 'raw_train_id'

##### Drop cols and rows, Convert data types
"""

### filter out cols ###

# density of Nans in cols
bin_chart(cols_info_df(raw_train_trans)['%_Nans'], '% of the values in the column are Nans', '# of cols')

# filter out the columns with more than 40% Nans.
cols_info_id = cols_info_df(raw_train_id)
cols_drop_id = cols_info_id[cols_info_id['%_Nans'] > 40]
cols_drop_id_list =  cols_drop_id.index.to_list() # list of columns to drop

if len(cols_drop_id_list) is not 0:
    print('The following columnes has more than 40% of Nan values:\n{}'.format(cols_drop_id_list))
    print('We suggest to drop all these columns in the dataset.')
else:
     print('We suggest no columns need to be dropped')

### filter out rows ###

df_row_null_id = null_info(raw_train_id, axis=1)

# density of Nans in rows
bin_chart(df_row_null_id['%_Nans'], '% of the values in the row are Nans', '# of rows')

# filter out the rows with more than 50% Nans.
rows_drop_id = df_row_null_id[df_row_null_id['%_Nans'] > 50]
rows_drop_id_list =  rows_drop_id.index.to_list() # list of rows to drop

if len(rows_drop_id_list) is not 0:
     print('In {} rows, there are more than 50% of values are null.\n'.format(len(rows_drop_id_list)))
     print('We suggest to drop all these rows in the dataset.')
else:
     print('We suggest no rows need to be dropped')

### filter out cols need convert dtypes ###

# count the unique values in the col with data type of 'object',
# see if it can be converted into 'category' type.
cols_obj_id = cols_info_id[cols_info_id['data_type'] == 'object'] # filter out cols with datatype = 'object'

cols_obj_id_list = cols_obj_id.index.to_list() # make the col names to a list

for col in cols_obj_id_list:
     cols_obj_id.loc[col, '#_uniques'] = raw_train_id[col].nunique() # count and append # of uniques

print('The datatype of these cols are object:\n',cols_obj_id,'\n')
# print(cols_obj_id_list)

# number of uniques in each cols are much small than the number of rows(140K+),
# all the 'object' cols can be considered as categorical cols.

### clean dataset with info and analysis above ###

clean_train_id = clean_df(raw_train_id, cols_drop_id_list, rows_drop_id_list, cols_obj_id_list) # drop cols and rows

# clean_train_id = get_dummies(clean_train_id, cols_obj_id_list) # get dummies

# Plot how the nan values looks after dropping part of the rows and cols.
msno.matrix(clean_train_id)

# Plot the distribution of percentage of nan values after dropping part of the cols and rows.

# density of Nans in cols
bin_chart(null_info(clean_train_id, axis=0)['%_Nans'], '% of the values in the column are Nans', '# of cols')

# density of Nans in rows
bin_chart(null_info(clean_train_id, axis=1)['%_Nans'], '% of the values in the row are Nans', '# of rows')

"""#### Clean 'raw_train_trans'.




"""

### filter out cols ###

# density of Nans in cols
bin_chart(null_info(raw_train_trans, axis=0)['%_Nans'], '% of the values in the column are Nans', '# of cols')

# filter out the columns with more than 60% Nans.
cols_info_trans = cols_info_df(raw_train_trans)
cols_drop_trans = cols_info_trans[cols_info_trans['%_Nans'] > 60]
cols_drop_trans_list =  cols_drop_trans.index.to_list() # list of columns to drop

if len(cols_drop_trans_list) is not 0:
    print('In {} columns, there are more than 60% of values are null.\n'.format(len(cols_drop_trans_list)))
    print('The following columnes has more than 60% of Nan values:\n{}'.format(cols_drop_trans_list))
    print('We suggest to drop all these columns in the dataset.')
else:
    print('We suggest no columns need to be dropped')

cols_drop_trans_list[:10]

# With the discussion above, 
# Updated 'cols_drop_trans_list'.
cols_drop_trans_list = ['dist2']

### filter out cols ###

# will do it after fill all the Nans in cols.

### filter out cols need convert dtypes ###

# count the unique values in the col with data type of 'object',
# see if it can be converted into 'category' type.
cols_obj_trans = cols_info_trans[cols_info_trans['data_type'] == 'object'] # filter out cols with datatype = 'object'

cols_obj_trans_list = cols_obj_trans.index.to_list() # make the col names to a list

for col in cols_obj_trans_list:
    cols_obj_trans.loc[col, '#_uniques'] = raw_train_trans[col].nunique() # count and append # of uniques

print(cols_obj_trans)

print('The features of #_uniques:{}\n'.format(cols_obj_trans['#_uniques'].describe()))

# Since the max value of #_uniques is 60, and we have more than 540k records, 
# all the 'object' cols can be considered as categorical cols.

clean_train_trans = clean_df(raw_train_trans, cols_drop_trans_list, [], cols_obj_trans_list) # drop cols

clean_train_trans

"""#### Write 'clean_train_id' and 'clean_train_trans' to .csv

"""

# clean_train_id.to_csv('clean_train_id.csv')
# !cp clean_train_id.csv "drive/MyDrive/Capstone/Data/"

# clean_train_trans.to_csv('clean_train_trans.csv')
# !cp clean_train_trans.csv "drive/MyDrive/Capstone/Data/"

# !pip freeze